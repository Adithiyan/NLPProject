{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q2gInb4j2MC",
        "outputId": "7108c49d-ec2f-493d-e0da-c5aedf50e34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.6.0\n",
            "Uninstalling scikit-learn-1.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/scikit_learn-1.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/sklearn/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scikit-learn-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install scikit-learn==1.5.2\n",
        "!pip install gensim plotly==4.14.3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import spacy\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoWSORFYj2x4",
        "outputId": "88b34dbf-1f43-4eaf-ab16-0b997652fe89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
            "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-1.5.2\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting plotly==4.14.3\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting retrying>=1.3.3 (from plotly==4.14.3)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from plotly==4.14.3) (1.17.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: retrying, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "Successfully installed plotly-4.14.3 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import defaultdict, Counter\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Paths\n",
        "USER_STORIES = [\n",
        "    ('Camperplus', 'camperplus.txt'),\n",
        "    ('Fish&Chips', 'Fish_Chips.txt'),\n",
        "    ('Grocery', 'grocery.txt'),\n",
        "    ('Planningpoker', 'planningpoker.txt'),\n",
        "    ('Recycling', 'recycling.txt'),\n",
        "    ('School', 'school.txt'),\n",
        "    ('Sports', 'sports.txt'),\n",
        "    ('Supermarket', 'supermarket.txt'),\n",
        "    ('Ticket', 'Ticket.txt')\n",
        "]\n",
        "GOLD_STANDARD_CLASSES = 'Gold standard - Classes.xlsx'\n",
        "GOLD_STANDARD_ASSOCIATIONS = 'Gold standard - Associations.xlsx'\n",
        "CLASS_OUTPUT = 'class_features.csv'\n",
        "ASSOCIATION_OUTPUT = 'association_features.csv'\n",
        "\n",
        "# Helper function to read files with fallback encoding\n",
        "def read_file_with_encoding(file_path):\n",
        "    \"\"\"Read a file with fallback encoding.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, encoding='utf-8') as f:\n",
        "            return f.readlines()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, encoding='cp1252') as f:\n",
        "            return f.readlines()\n",
        "\n",
        "# Extract features for class identification\n",
        "def extract_features_for_classes(user_story_path, gold_standard_classes_path, class_column):\n",
        "    \"\"\"Extract features for class identification.\"\"\"\n",
        "    if not os.path.exists(user_story_path):\n",
        "        print(f\"Error: User story file not found -> {user_story_path}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(gold_standard_classes_path):\n",
        "        print(f\"Error: Gold standard classes file not found -> {gold_standard_classes_path}\")\n",
        "        return\n",
        "\n",
        "    # Read user stories\n",
        "    lines = read_file_with_encoding(user_story_path)\n",
        "\n",
        "    # Read gold standard\n",
        "    gold_standard = pd.read_excel(gold_standard_classes_path, engine='openpyxl')\n",
        "    if class_column not in gold_standard.columns:\n",
        "        print(f\"Error: Column '{class_column}' not found in Gold Standard Classes.\")\n",
        "        return\n",
        "\n",
        "    class_labels = set(stemmer.stem(label.lower()) for label in gold_standard[class_column].dropna())\n",
        "\n",
        "    entities = defaultdict(lambda: {\n",
        "        'count': 0, 'noun': 0, 'subject': 0, 'compound': 0, 'gerund': 0,\n",
        "        'user_role': 0, 'action': 0, 'benefit': 0, 'part_of_frequent_ngram': 0, 'target': 0\n",
        "    })\n",
        "\n",
        "    global_ngrams = Counter()\n",
        "\n",
        "    for line in lines:\n",
        "        doc = nlp(line)\n",
        "        words = [token.text.lower() for token in doc if token.text.lower() not in stop_words]\n",
        "\n",
        "        # Collect n-grams globally across all lines\n",
        "        ngrams = [' '.join(words[i:i+n]) for n in range(2, 5) for i in range(len(words) - n + 1)]\n",
        "        global_ngrams.update(ngrams)\n",
        "\n",
        "        for token in doc:\n",
        "            word = stemmer.stem(token.text.lower())\n",
        "            if len(word) <= 2 or word in stop_words:\n",
        "                continue\n",
        "\n",
        "            pos = token.tag_\n",
        "            dep = token.dep_\n",
        "            entities[word]['count'] += 1\n",
        "            if word in class_labels:\n",
        "                entities[word]['target'] = 1\n",
        "            if 'NN' in pos:\n",
        "                entities[word]['noun'] += 1\n",
        "            if dep == 'nsubj':\n",
        "                entities[word]['subject'] += 1\n",
        "            if dep == 'compound':\n",
        "                entities[word]['compound'] += 1\n",
        "            if pos == 'VBG':\n",
        "                entities[word]['gerund'] += 1\n",
        "\n",
        "        # Extract role, action, benefit\n",
        "        if line.lower().startswith(\"as a \"):\n",
        "            parts = line.split(\",\")\n",
        "            if len(parts) >= 3:\n",
        "                role = parts[0][5:].strip()\n",
        "                action = parts[1][8:].strip()\n",
        "                benefit = parts[2][8:].strip()\n",
        "                for token in nlp(role):\n",
        "                    entities[token.text.lower()]['user_role'] += 1\n",
        "                for token in nlp(action):\n",
        "                    entities[token.text.lower()]['action'] += 1\n",
        "                for token in nlp(benefit):\n",
        "                    entities[token.text.lower()]['benefit'] += 1\n",
        "\n",
        "    # Update entities with global n-gram frequencies\n",
        "    for ngram, freq in global_ngrams.items():\n",
        "        if freq >= 2:\n",
        "            entities[ngram]['part_of_frequent_ngram'] = 1\n",
        "\n",
        "    data = pd.DataFrame.from_dict(entities, orient='index')\n",
        "    data.index.name = 'phrase'\n",
        "    data.reset_index(inplace=True)\n",
        "    data.to_csv(CLASS_OUTPUT.replace('features.csv', f'{class_column}_features.csv'), index=False)\n",
        "    print(f\"Class features for {class_column} saved.\")\n",
        "\n",
        "# Extract features for association identification\n",
        "def extract_features_for_associations(user_story_path, gold_standard_assoc_path, project_name):\n",
        "    \"\"\"Extract features for association identification.\"\"\"\n",
        "    if not os.path.exists(user_story_path):\n",
        "        print(f\"Error: User story file not found -> {user_story_path}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(gold_standard_assoc_path):\n",
        "        print(f\"Error: Gold standard associations file not found -> {gold_standard_assoc_path}\")\n",
        "        return\n",
        "\n",
        "    # Load gold standard associations\n",
        "    gold_standard = pd.read_excel(gold_standard_assoc_path, engine='openpyxl')\n",
        "    if project_name not in gold_standard.columns or f\"Unnamed: {gold_standard.columns.get_loc(project_name)+1}\" not in gold_standard.columns:\n",
        "        print(f\"Error: Columns for project '{project_name}' not found in Gold Standard Associations.\")\n",
        "        return\n",
        "\n",
        "    associations = set(\n",
        "        zip(\n",
        "            gold_standard[project_name].dropna().str.lower(),\n",
        "            gold_standard[f\"Unnamed: {gold_standard.columns.get_loc(project_name)+1}\"].dropna().str.lower()\n",
        "        )\n",
        "    )\n",
        "\n",
        "    classes = {assoc[0] for assoc in associations} | {assoc[1] for assoc in associations}\n",
        "\n",
        "    # Initialize features\n",
        "    features = []\n",
        "    for class_a in classes:\n",
        "        for class_b in classes:\n",
        "            if class_a == class_b:\n",
        "                continue\n",
        "\n",
        "            # External Knowledge: Similarity\n",
        "            similarity = cosine_similarity([nlp(class_a).vector], [nlp(class_b).vector])[0][0]\n",
        "\n",
        "            # Internal Knowledge: Co-occurrence\n",
        "            cooccurrence = sum(1 for line in read_file_with_encoding(user_story_path)\n",
        "                               if class_a in line and class_b in line)\n",
        "\n",
        "            # Internal Knowledge: Individual counts\n",
        "            count_a = sum(1 for line in read_file_with_encoding(user_story_path) if class_a in line)\n",
        "            count_b = sum(1 for line in read_file_with_encoding(user_story_path) if class_b in line)\n",
        "\n",
        "            # Requirements-Type Knowledge: Role, Action, Benefit\n",
        "            role_overlap = 0\n",
        "            action_overlap = 0\n",
        "            benefit_overlap = 0\n",
        "            for line in read_file_with_encoding(user_story_path):\n",
        "                if line.lower().startswith(\"as a \"):\n",
        "                    parts = line.split(\",\")\n",
        "                    if len(parts) >= 3:\n",
        "                        role = parts[0][5:].strip()\n",
        "                        action = parts[1][8:].strip()\n",
        "                        benefit = parts[2][8:].strip()\n",
        "                        if class_a in role and class_b in role:\n",
        "                            role_overlap += 1\n",
        "                        if class_a in action and class_b in action:\n",
        "                            action_overlap += 1\n",
        "                        if class_a in benefit and class_b in benefit:\n",
        "                            benefit_overlap += 1\n",
        "\n",
        "            # Add feature dictionary\n",
        "            features.append({\n",
        "                'class_a': class_a,\n",
        "                'class_b': class_b,\n",
        "                'similarity': similarity,\n",
        "                'cooccurrence': cooccurrence,\n",
        "                'count_a': count_a,\n",
        "                'count_b': count_b,\n",
        "                'role_overlap': role_overlap,\n",
        "                'action_overlap': action_overlap,\n",
        "                'benefit_overlap': benefit_overlap,\n",
        "                'target': int((class_a, class_b) in associations or (class_b, class_a) in associations)\n",
        "            })\n",
        "\n",
        "    # Save features to a CSV\n",
        "    feature_df = pd.DataFrame(features)\n",
        "    feature_df.to_csv(ASSOCIATION_OUTPUT.replace('features.csv', f'{project_name}_features.csv'), index=False)\n",
        "    print(f\"Association features for {project_name} saved.\")\n",
        "\n",
        "# Main Processing Loop\n",
        "for project_name, user_story_file in USER_STORIES:\n",
        "    print(f\"\\nProcessing project: {project_name}\")\n",
        "    extract_features_for_classes(user_story_file, GOLD_STANDARD_CLASSES, project_name)\n",
        "    extract_features_for_associations(user_story_file, GOLD_STANDARD_ASSOCIATIONS, project_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8703kwTkPXB",
        "outputId": "800d24b6-457f-4515-eddd-d432cfa02f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing project: Camperplus\n",
            "Class features for Camperplus saved.\n",
            "Association features for Camperplus saved.\n",
            "\n",
            "Processing project: Fish&Chips\n",
            "Class features for Fish&Chips saved.\n",
            "Association features for Fish&Chips saved.\n",
            "\n",
            "Processing project: Grocery\n",
            "Class features for Grocery saved.\n",
            "Association features for Grocery saved.\n",
            "\n",
            "Processing project: Planningpoker\n",
            "Class features for Planningpoker saved.\n",
            "Association features for Planningpoker saved.\n",
            "\n",
            "Processing project: Recycling\n",
            "Class features for Recycling saved.\n",
            "Association features for Recycling saved.\n",
            "\n",
            "Processing project: School\n",
            "Class features for School saved.\n",
            "Association features for School saved.\n",
            "\n",
            "Processing project: Sports\n",
            "Class features for Sports saved.\n",
            "Association features for Sports saved.\n",
            "\n",
            "Processing project: Supermarket\n",
            "Class features for Supermarket saved.\n",
            "Association features for Supermarket saved.\n",
            "\n",
            "Processing project: Ticket\n",
            "Class features for Ticket saved.\n",
            "Association features for Ticket saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eiR6m87kjkr",
        "outputId": "8d56363a-6889-4d17-ac4c-7832003ecfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, Input\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "def balance_data(X, y):\n",
        "    ros = RandomOverSampler(random_state=21)\n",
        "    X_res, y_res = ros.fit_resample(X, y)\n",
        "    return X_res, y_res\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(Conv1D(64, kernel_size=min(3, input_shape[0]), activation='relu'))\n",
        "    if input_shape[0] > 1:\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    model.add(LSTM(64, return_sequences=False, dropout=0.5, recurrent_dropout=0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "def train_models(features, target_col, task_name):\n",
        "    features = features.select_dtypes(include=[np.number])\n",
        "    if target_col not in features.columns:\n",
        "        print(f\"Error: Target column '{target_col}' not found in features for {task_name}.\")\n",
        "        return\n",
        "\n",
        "    X = features.drop(columns=[target_col])\n",
        "    y = features[target_col]\n",
        "\n",
        "    if y.nunique() == 1:\n",
        "        print(f\"Skipping {task_name}: Only one class present. Model training not possible.\")\n",
        "        return\n",
        "\n",
        "    X_balanced, y_balanced = balance_data(X, y)\n",
        "\n",
        "    models = {\n",
        "    'Logistic Regression': (\n",
        "        LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs'),\n",
        "        {\n",
        "            'C': [0.01, 0.1, 1, 10]\n",
        "        }\n",
        "    ),\n",
        "    'Random Forest': (\n",
        "        RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "        {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 10, 20]\n",
        "        }\n",
        "    ),\n",
        "    'Gradient Boosting': (\n",
        "        GradientBoostingClassifier(random_state=30),\n",
        "        {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1]\n",
        "        }\n",
        "    ),\n",
        "    'SVM': (\n",
        "        SVC(probability=True, class_weight='balanced', random_state=20),\n",
        "        {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    ),\n",
        "    'MLP': (\n",
        "        MLPClassifier(max_iter=1000, random_state=15, early_stopping=True, validation_fraction=0.2),\n",
        "        {\n",
        "            'hidden_layer_sizes': [(100,), (50, 50)],\n",
        "            'alpha': [0.0001, 0.001]\n",
        "        }\n",
        "    ),\n",
        "    'Naive Bayes': (\n",
        "        GaussianNB(),\n",
        "        {}\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "    base_learners = [(name, model[0]) for name, model in models.items() if name != 'MLP']\n",
        "    stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression(max_iter=1000))\n",
        "    models['Stacking Classifier'] = (stacking_clf, {})\n",
        "\n",
        "    cnn_model = KerasClassifier(model=create_cnn_model, input_shape=(X_balanced.shape[1], 1), epochs=10, batch_size=32, verbose=0)\n",
        "    lstm_model = KerasClassifier(model=create_lstm_model, input_shape=(X_balanced.shape[1], 1), epochs=10, batch_size=32, verbose=0)\n",
        "    models['CNN'] = (cnn_model, {})\n",
        "    models['LSTM'] = (lstm_model, {})\n",
        "\n",
        "    results = []\n",
        "    skf = StratifiedKFold(n_splits=8, shuffle=True)\n",
        "\n",
        "    for name, (model, param_grid) in models.items():\n",
        "        f0_5_scores, f1_scores, f2_scores = [], [], []\n",
        "        precision_scores, recall_scores = [], []\n",
        "\n",
        "        for train_idx, test_idx in skf.split(X_balanced, y_balanced):\n",
        "            X_train, X_test = X_balanced.iloc[train_idx], X_balanced.iloc[test_idx]\n",
        "            y_train, y_test = y_balanced.iloc[train_idx], y_balanced.iloc[test_idx]\n",
        "\n",
        "            scaler = MinMaxScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            if name in ['CNN', 'LSTM']:\n",
        "                X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "                X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "            if param_grid:\n",
        "                model = tune_hyperparameters(model, param_grid, X_train_scaled, y_train)\n",
        "\n",
        "            pipeline = Pipeline([\n",
        "                ('model', model)\n",
        "            ])\n",
        "            pipeline.fit(X_train_scaled, y_train)\n",
        "            y_pred = pipeline.predict(X_test_scaled)\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
        "            f0_5 = (1 + 0.5 ** 2) * (precision * recall) / ((0.5 ** 2 * precision) + recall) if (precision + recall) > 0 else 0\n",
        "            f2 = (1 + 2 ** 2) * (precision * recall) / ((2 ** 2 * precision) + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            f0_5_scores.append(f0_5)\n",
        "            f1_scores.append(f1)\n",
        "            f2_scores.append(f2)\n",
        "            precision_scores.append(precision)\n",
        "            recall_scores.append(recall)\n",
        "\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Precision': np.mean(precision_scores),\n",
        "            'Recall': np.mean(recall_scores),\n",
        "            'F0.5': np.mean(f0_5_scores),\n",
        "            'F1': np.mean(f1_scores),\n",
        "            'F2': np.mean(f2_scores)\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"\\n{task_name} Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    project_files = [\n",
        "        (\"Camperplus\", \"class_Camperplus_features.csv\", \"association_Camperplus_features.csv\"),\n",
        "        (\"Fish&Chips\", \"class_Fish&Chips_features.csv\", \"association_Fish&Chips_features.csv\"),\n",
        "        (\"Grocery\", \"class_Grocery_features.csv\", \"association_Grocery_features.csv\"),\n",
        "        (\"Planningpoker\", \"class_Planningpoker_features.csv\", \"association_Planningpoker_features.csv\"),\n",
        "        (\"Recycling\", \"class_Recycling_features.csv\", \"association_Recycling_features.csv\"),\n",
        "        (\"School\", \"class_School_features.csv\", \"association_School_features.csv\"),\n",
        "        (\"Sports\", \"class_Sports_features.csv\", \"association_Sports_features.csv\"),\n",
        "        (\"Supermarket\", \"class_Supermarket_features.csv\", \"association_Supermarket_features.csv\"),\n",
        "        (\"Ticket\", \"class_Ticket_features.csv\", \"association_Ticket_features.csv\")\n",
        "    ]\n",
        "\n",
        "    for project_name, class_file, assoc_file in project_files:\n",
        "        print(f\"\\nProcessing project: {project_name}\")\n",
        "\n",
        "        if os.path.exists(class_file):\n",
        "            class_features = pd.read_csv(class_file)\n",
        "            print(f\"\\nClass Identification Models for {project_name}:\")\n",
        "            train_models(class_features, 'target', f\"{project_name} Class Identification\")\n",
        "        else:\n",
        "            print(f\"Skipping: Class features file not found -> {class_file}\")\n",
        "\n",
        "        if os.path.exists(assoc_file):\n",
        "            assoc_features = pd.read_csv(assoc_file)\n",
        "            print(f\"\\nAssociation Identification Models for {project_name}:\")\n",
        "            train_models(assoc_features, 'target', f\"{project_name} Association Identification\")\n",
        "        else:\n",
        "            print(f\"Skipping: Association features file not found -> {assoc_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU04VXrMk2Hn",
        "outputId": "c5dd2820-7cd8-4dfa-8a85-ef8429af36e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing project: Camperplus\n",
            "\n",
            "Class Identification Models for Camperplus:\n",
            "\n",
            "Camperplus Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.887400  1.000000  0.907583  0.939886  0.974924\n",
            "1        Random Forest   0.955860  1.000000  0.964326  0.977356  0.990798\n",
            "2    Gradient Boosting   0.948301  1.000000  0.958134  0.973342  0.989132\n",
            "3                  SVM   0.856064  1.000000  0.881339  0.922274  0.967336\n",
            "4                  MLP   0.841497  0.680897  0.788783  0.732070  0.695676\n",
            "5          Naive Bayes   0.606603  1.000000  0.658281  0.754842  0.884880\n",
            "6  Stacking Classifier   0.953209  1.000000  0.961993  0.975682  0.990034\n",
            "7                  CNN   0.865361  0.596328  0.785639  0.696706  0.631212\n",
            "8                 LSTM   0.784272  0.820763  0.772941  0.774846  0.796007\n",
            "\n",
            "Association Identification Models for Camperplus:\n",
            "\n",
            "Camperplus Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.658918  0.567357  0.636422  0.607328  0.582309\n",
            "1        Random Forest   0.905859  0.991848  0.921548  0.946402  0.973016\n",
            "2    Gradient Boosting   0.954330  1.000000  0.963033  0.976479  0.990418\n",
            "3                  SVM   0.848338  0.755666  0.826954  0.797824  0.771681\n",
            "4                  MLP   0.740365  0.682354  0.726208  0.707728  0.691821\n",
            "5          Naive Bayes   0.779379  0.190738  0.473284  0.303054  0.223814\n",
            "6  Stacking Classifier   0.952439  0.986586  0.958740  0.968665  0.979201\n",
            "7                  CNN   0.751392  0.674838  0.732194  0.707461  0.686756\n",
            "8                 LSTM   0.816273  0.260754  0.556574  0.387482  0.299650\n",
            "\n",
            "Processing project: Fish&Chips\n",
            "\n",
            "Class Identification Models for Fish&Chips:\n",
            "\n",
            "Fish&Chips Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.947134  0.913306  0.937193  0.925689  0.917400\n",
            "1        Random Forest   0.996094  1.000000  0.996855  0.998016  0.999199\n",
            "2    Gradient Boosting   0.998047  1.000000  0.998433  0.999016  0.999604\n",
            "3                  SVM   0.949921  1.000000  0.959361  0.974039  0.989380\n",
            "4                  MLP   0.957112  0.744048  0.904531  0.836359  0.778278\n",
            "5          Naive Bayes   0.609523  1.000000  0.661087  0.757234  0.886262\n",
            "6  Stacking Classifier   1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "7                  CNN   0.953839  0.772433  0.909556  0.851616  0.801877\n",
            "8                 LSTM   0.922217  0.780274  0.883563  0.837208  0.800628\n",
            "\n",
            "Association Identification Models for Fish&Chips:\n",
            "\n",
            "Fish&Chips Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.644486  0.666258  0.643528  0.647964  0.657558\n",
            "1        Random Forest   0.973684  1.000000  0.978495  0.986111  0.994253\n",
            "2    Gradient Boosting   0.981250  1.000000  0.984536  0.989865  0.995739\n",
            "3                  SVM   0.705203  0.854984  0.725074  0.763321  0.813420\n",
            "4                  MLP   0.613668  0.448121  0.564804  0.511252  0.470660\n",
            "5          Naive Bayes   0.735440  0.818219  0.749674  0.773000  0.799066\n",
            "6  Stacking Classifier   0.987500  1.000000  0.989796  0.993421  0.997283\n",
            "7                  CNN   0.599194  0.639706  0.605439  0.616427  0.629579\n",
            "8                 LSTM   0.579306  0.636846  0.577829  0.588190  0.612158\n",
            "\n",
            "Processing project: Grocery\n",
            "\n",
            "Class Identification Models for Grocery:\n",
            "\n",
            "Grocery Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.958472  0.772917  0.911854  0.852363  0.802276\n",
            "1        Random Forest   0.934765  1.000000  0.947054  0.966170  0.986158\n",
            "2    Gradient Boosting   0.931211  1.000000  0.944140  0.964280  0.985372\n",
            "3                  SVM   0.912292  1.000000  0.928452  0.953918  0.980985\n",
            "4                  MLP   0.926756  0.759904  0.880756  0.825263  0.782494\n",
            "5          Naive Bayes   0.656544  1.000000  0.704857  0.792416  0.905043\n",
            "6  Stacking Classifier   0.937056  1.000000  0.948858  0.967265  0.986580\n",
            "7                  CNN   0.947838  0.725068  0.890776  0.818982  0.759495\n",
            "8                 LSTM   0.919325  0.755874  0.879719  0.827645  0.782655\n",
            "\n",
            "Association Identification Models for Grocery:\n",
            "\n",
            "Grocery Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.962912  0.868371  0.933453  0.901260  0.879072\n",
            "1        Random Forest   0.960737  1.000000  0.967840  0.979149  0.991333\n",
            "2    Gradient Boosting   0.959936  1.000000  0.967426  0.979130  0.991437\n",
            "3                  SVM   0.958042  0.859848  0.934407  0.903111  0.875980\n",
            "4                  MLP   0.799937  0.901515  0.817036  0.845298  0.877414\n",
            "5          Naive Bayes   0.640286  1.000000  0.689530  0.779820  0.898134\n",
            "6  Stacking Classifier   0.970353  1.000000  0.975900  0.984565  0.993669\n",
            "7                  CNN   0.728564  0.911932  0.756155  0.804136  0.863231\n",
            "8                 LSTM   0.661451  0.920455  0.689487  0.750230  0.838775\n",
            "\n",
            "Processing project: Planningpoker\n",
            "\n",
            "Class Identification Models for Planningpoker:\n",
            "\n",
            "Planningpoker Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.984949  1.000000  0.987862  0.992320  0.996890\n",
            "1        Random Forest   0.995332  1.000000  0.996256  0.997651  0.999057\n",
            "2    Gradient Boosting   0.996875  1.000000  0.997494  0.998428  0.999369\n",
            "3                  SVM   0.980126  1.000000  0.984002  0.989907  0.995924\n",
            "4                  MLP   0.979717  0.839280  0.946922  0.902589  0.863197\n",
            "5          Naive Bayes   0.586738  1.000000  0.639574  0.739483  0.876452\n",
            "6  Stacking Classifier   0.992245  1.000000  0.993775  0.996089  0.998427\n",
            "7                  CNN   0.978951  1.000000  0.983014  0.989242  0.995638\n",
            "8                 LSTM   0.974213  0.812401  0.936422  0.885361  0.839978\n",
            "\n",
            "Association Identification Models for Planningpoker:\n",
            "\n",
            "Planningpoker Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.908333  0.741667  0.859068  0.803409  0.762769\n",
            "1        Random Forest   0.922619  0.975000  0.929262  0.942551  0.960292\n",
            "2    Gradient Boosting   0.968750  1.000000  0.973684  0.982143  0.992188\n",
            "3                  SVM   0.876042  0.900000  0.875551  0.879235  0.889091\n",
            "4                  MLP   0.491667  0.441667  0.472167  0.454419  0.444925\n",
            "5          Naive Bayes   0.877083  0.662500  0.813227  0.743777  0.691446\n",
            "6  Stacking Classifier   0.896577  1.000000  0.911915  0.938957  0.972540\n",
            "7                  CNN   0.701786  0.775000  0.692342  0.700379  0.734121\n",
            "8                 LSTM   0.464583  0.566667  0.465187  0.482479  0.522040\n",
            "\n",
            "Processing project: Recycling\n",
            "\n",
            "Class Identification Models for Recycling:\n",
            "\n",
            "Recycling Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.914504  0.859405  0.901377  0.883729  0.868395\n",
            "1        Random Forest   0.899843  1.000000  0.918152  0.947136  0.978121\n",
            "2    Gradient Boosting   0.905802  1.000000  0.922944  0.950143  0.979323\n",
            "3                  SVM   0.887682  1.000000  0.907958  0.940287  0.975167\n",
            "4                  MLP   0.786546  0.963462  0.813504  0.860591  0.917878\n",
            "5          Naive Bayes   0.584917  1.000000  0.637839  0.738026  0.875627\n",
            "6  Stacking Classifier   0.901716  1.000000  0.919674  0.948110  0.978519\n",
            "7                  CNN   0.919615  0.849249  0.903612  0.881564  0.861571\n",
            "8                 LSTM   0.875113  0.810968  0.860198  0.839973  0.821942\n",
            "\n",
            "Association Identification Models for Recycling:\n",
            "\n",
            "Recycling Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.852679  0.810268  0.840977  0.826786  0.815927\n",
            "1        Random Forest   0.937500  1.000000  0.948718  0.966667  0.986111\n",
            "2    Gradient Boosting   1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "3                  SVM   0.843998  0.810268  0.833272  0.821264  0.813437\n",
            "4                  MLP   0.647321  0.584821  0.626559  0.604825  0.590740\n",
            "5          Naive Bayes   0.681597  0.899554  0.714804  0.772514  0.842888\n",
            "6  Stacking Classifier   1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "7                  CNN   0.583127  1.000000  0.635120  0.734187  0.872298\n",
            "8                 LSTM   0.596591  0.904018  0.628847  0.696569  0.800456\n",
            "\n",
            "Processing project: School\n",
            "\n",
            "Class Identification Models for School:\n",
            "\n",
            "School Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.949272  1.000000  0.958846  0.973720  0.989253\n",
            "1        Random Forest   0.976726  1.000000  0.981257  0.988166  0.995217\n",
            "2    Gradient Boosting   0.976753  1.000000  0.981270  0.988167  0.995215\n",
            "3                  SVM   0.953432  1.000000  0.962276  0.975967  0.990196\n",
            "4                  MLP   0.931648  1.000000  0.944488  0.964494  0.985456\n",
            "5          Naive Bayes   0.648109  1.000000  0.696946  0.785972  0.901545\n",
            "6  Stacking Classifier   0.979819  1.000000  0.983747  0.989739  0.995854\n",
            "7                  CNN   0.952047  0.980769  0.957416  0.965816  0.974642\n",
            "8                 LSTM   0.932085  0.919705  0.928959  0.924899  0.921555\n",
            "\n",
            "Association Identification Models for School:\n",
            "\n",
            "School Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.662227  0.656062  0.658735  0.655670  0.655094\n",
            "1        Random Forest   0.947114  1.000000  0.956920  0.972310  0.988596\n",
            "2    Gradient Boosting   0.952966  1.000000  0.961820  0.975594  0.990007\n",
            "3                  SVM   0.830623  0.778810  0.818977  0.802783  0.787977\n",
            "4                  MLP   0.670093  0.742305  0.675714  0.692514  0.718968\n",
            "5          Naive Bayes   0.616472  0.860830  0.649411  0.710327  0.790915\n",
            "6  Stacking Classifier   0.980174  0.989677  0.981940  0.984716  0.987643\n",
            "7                  CNN   0.671798  0.575263  0.647494  0.616469  0.590437\n",
            "8                 LSTM   0.635947  0.296359  0.489817  0.386448  0.325614\n",
            "\n",
            "Processing project: Sports\n",
            "\n",
            "Class Identification Models for Sports:\n",
            "\n",
            "Sports Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.944898  0.995614  0.954552  0.969476  0.984961\n",
            "1        Random Forest   0.971199  1.000000  0.976721  0.985221  0.993993\n",
            "2    Gradient Boosting   0.970825  1.000000  0.976483  0.985132  0.993983\n",
            "3                  SVM   0.942613  0.987069  0.950924  0.963917  0.977575\n",
            "4                  MLP   0.867526  1.000000  0.889944  0.926902  0.968739\n",
            "5          Naive Bayes   0.684111  1.000000  0.730158  0.812240  0.915283\n",
            "6  Stacking Classifier   0.970931  1.000000  0.976535  0.985133  0.993970\n",
            "7                  CNN   0.941470  0.930619  0.939001  0.935584  0.932498\n",
            "8                 LSTM   0.907783  0.954288  0.916164  0.929558  0.944014\n",
            "\n",
            "Association Identification Models for Sports:\n",
            "\n",
            "Sports Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.742199  0.709539  0.733561  0.722656  0.714051\n",
            "1        Random Forest   0.910797  1.000000  0.926619  0.952091  0.979933\n",
            "2    Gradient Boosting   0.981548  1.000000  0.985091  0.990541  0.996158\n",
            "3                  SVM   0.787951  0.897697  0.806502  0.837160  0.871860\n",
            "4                  MLP   0.697119  0.708224  0.692222  0.692396  0.699726\n",
            "5          Naive Bayes   0.516803  1.000000  0.572024  0.681264  0.842254\n",
            "6  Stacking Classifier   0.981845  1.000000  0.985334  0.990697  0.996223\n",
            "7                  CNN   0.706072  0.744737  0.710016  0.719157  0.732737\n",
            "8                 LSTM   0.602792  0.553618  0.562667  0.539768  0.541420\n",
            "\n",
            "Processing project: Supermarket\n",
            "\n",
            "Class Identification Models for Supermarket:\n",
            "\n",
            "Supermarket Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.959636  0.865042  0.938501  0.909039  0.881943\n",
            "1        Random Forest   0.969018  1.000000  0.974964  0.984110  0.993544\n",
            "2    Gradient Boosting   0.964569  1.000000  0.971385  0.981854  0.992633\n",
            "3                  SVM   0.963920  0.864998  0.942187  0.911531  0.882978\n",
            "4                  MLP   0.958174  0.772187  0.907549  0.847070  0.799147\n",
            "5          Naive Bayes   0.603284  1.000000  0.655149  0.752262  0.883460\n",
            "6  Stacking Classifier   0.967026  1.000000  0.973302  0.983006  0.993074\n",
            "7                  CNN   0.949944  0.762972  0.902431  0.842301  0.792151\n",
            "8                 LSTM   0.954143  0.709513  0.891113  0.812104  0.747001\n",
            "\n",
            "Association Identification Models for Supermarket:\n",
            "\n",
            "Supermarket Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.896293  0.914062  0.898956  0.903780  0.909607\n",
            "1        Random Forest   0.966374  1.000000  0.972332  0.981933  0.992433\n",
            "2    Gradient Boosting   0.986111  1.000000  0.988636  0.992647  0.996951\n",
            "3                  SVM   0.957285  0.984375  0.962118  0.969873  0.978314\n",
            "4                  MLP   0.793269  0.898438  0.810948  0.840264  0.873568\n",
            "5          Naive Bayes   0.714987  0.937500  0.749271  0.808731  0.880582\n",
            "6  Stacking Classifier   0.986111  1.000000  0.988636  0.992647  0.996951\n",
            "7                  CNN   0.775245  1.000000  0.811164  0.872273  0.944269\n",
            "8                 LSTM   0.880792  0.679688  0.815787  0.752213  0.705629\n",
            "\n",
            "Processing project: Ticket\n",
            "\n",
            "Class Identification Models for Ticket:\n",
            "\n",
            "Ticket Class Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.946110  0.777542  0.906340  0.852998  0.805959\n",
            "1        Random Forest   0.979440  1.000000  0.983454  0.989566  0.995788\n",
            "2    Gradient Boosting   0.979440  1.000000  0.983454  0.989566  0.995788\n",
            "3                  SVM   0.890856  1.000000  0.910692  0.942202  0.976029\n",
            "4                  MLP   0.958253  0.777542  0.915344  0.858044  0.807778\n",
            "5          Naive Bayes   0.859946  0.889831  0.863775  0.871495  0.881645\n",
            "6  Stacking Classifier   0.979505  1.000000  0.983497  0.989583  0.995791\n",
            "7                  CNN   0.938618  0.777542  0.900883  0.850005  0.804898\n",
            "8                 LSTM   0.920376  0.796610  0.890488  0.850977  0.816833\n",
            "\n",
            "Association Identification Models for Ticket:\n",
            "\n",
            "Ticket Association Identification Results:\n",
            "                 Model  Precision    Recall      F0.5        F1        F2\n",
            "0  Logistic Regression   0.621502  0.613248  0.617573  0.613836  0.612635\n",
            "1        Random Forest   0.952456  1.000000  0.961337  0.975216  0.989823\n",
            "2    Gradient Boosting   0.991379  1.000000  0.993007  0.995536  0.998175\n",
            "3                  SVM   0.811190  0.901709  0.826483  0.851863  0.880529\n",
            "4                  MLP   0.625827  0.665776  0.629277  0.638659  0.653110\n",
            "5          Naive Bayes   0.494100  0.853454  0.539364  0.625451  0.744640\n",
            "6  Stacking Classifier   0.991379  1.000000  0.993007  0.995536  0.998175\n",
            "7                  CNN   0.667978  0.895477  0.703393  0.764498  0.837809\n",
            "8                 LSTM   0.692397  0.488248  0.616172  0.548480  0.507433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZP-WYfP9lAxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}